{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elqm.utils.dataFinder import get_data_dir\n",
    "from elqm.backend.utils import get_es_connection\n",
    "import os\n",
    "import time\n",
    "\n",
    "DATA_DIR = os.path.abspath(get_data_dir(\"eur_lex_data\"))\n",
    "PREPROCESSED_DATA_DIR = os.path.abspath(get_data_dir(\"preprocessed\"))\n",
    "\n",
    "print(\"DATA_DIR: \", DATA_DIR)\n",
    "print(\"PREPROCESSED_DATA_DIR: \", PREPROCESSED_DATA_DIR)\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Initilize the LLM model\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "questionPrompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"Answer the question based only on the following context and on the conversation history:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "systemString = \"\"\"You are ELQM, a helpful and specialized assistant for question-answering tasks \\\n",
    "in the domain of energy law. Use the following pieces of retrieved context comprised of EU \\\n",
    "regulations and other legal documents to answer the question. If you don't know the answer \\\n",
    "or the question cannot be answered with the context, admit that you cannot answer the \\\n",
    "question due to the limited available context. Furthermore, if the user asks a generic \\\n",
    "question or other situations occur, in which the context is not helpful, kindly remember the \\\n",
    "user of your purpose. Your answers should not include any racist, sexist and toxic content.\"\"\"\n",
    "\n",
    "systemMessage = SystemMessage(content=systemString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "historyPrompt = MessagesPlaceholder(variable_name=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = (systemMessage + historyPrompt + questionPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt.format_messages(context=\"Hello\", question=\"Test\", history=\"This is the history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "embeddings = GPT4AllEmbeddings();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n",
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\",\n",
    "     \"Josh worked at John Deere\",\n",
    "     \"Sabrina worked at Google\",\n",
    "     \"Jasmin worked at Continental\",\n",
    "     \"James worked at Microsoft\",\n",
    "     \"Joshua worked at the local mechanics shop\",\n",
    "     \"Nikita worked at Rechner Sensors\",\n",
    "     \"Nikita worked at the University\",\n",
    "     \"Kira worked at the school\",\n",
    "     \"Uli worked at the airport\",\n",
    "     \"Maria worked at Mitsubishi Chemical\"],\n",
    "    embedding=GPT4AllEmbeddings()\n",
    ")\n",
    "\n",
    "# As default the retreiver outputs 4 documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where did Harrison work?\"\n",
    "docs = retriever.invoke(query)\n",
    "print(type(docs))\n",
    "print(\"Number of docs:\", len(docs))\n",
    "print()\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i}\")\n",
    "    print(\"Content\", doc.page_content)\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug output is good good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following two seem so be equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain2 = setup_and_retrieval | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to make a class with different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "result = setup_and_retrieval.invoke(\"Where did Harrison work?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "setup_and_retrieval_runnable = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "@chain\n",
    "def custom_chain(question):\n",
    "    retrival_output = setup_and_retrieval.invoke(question)\n",
    "    prompt_output = prompt.invoke(retrival_output)\n",
    "    llm_output = llm.invoke(prompt_output)\n",
    "    anwser = StrOutputParser().invoke(llm_output)\n",
    "    return anwser, retrival_output, prompt_output, llm_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chain.invoke(\"Where did Harrison work and what did he do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class is nice but it is crap that the chain here does not show because it is defined explicetly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a parallel pass through that passes retrieved data all the way through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_core.prompt_values\n",
    "\n",
    "def clean_retriever_string(retrievedDocuments: langchain_core.prompt_values.ChatPromptValue):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain31 = retriever\n",
    "\n",
    "chain32 = {\"context\": retriever,\n",
    "           \"question\": RunnablePassthrough()} | prompt\n",
    "\n",
    "chain33 = (llm | StrOutputParser())\n",
    "\n",
    "chain34 = (\n",
    "    chain32 | RunnableParallel(completly_processed=chain33,\n",
    "                               prompted=RunnablePassthrough())\n",
    ")\n",
    "\n",
    "chain35 = RunnableParallel(completly_and_prompted=chain34,\n",
    "                           retreived=chain31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain32.invoke(\"Where did Harrison work and what did he do?\")\n",
    "print(type(result))\n",
    "final_output = result.to_messages()\n",
    "print(type(final_output))\n",
    "print(len(final_output))\n",
    "print(final_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain35.invoke(\"Where did Harrison work and what did he do?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        +-------------------------------------------------+                   \n",
      "                        | Parallel<completly_and_prompted,retreived>Input |                   \n",
      "                        +-------------------------------------------------+                   \n",
      "                                     ****                      *****                          \n",
      "                                 ****                               *****                     \n",
      "                               **                                        *****                \n",
      "         +---------------------------------+                                  ***             \n",
      "         | Parallel<context,question>Input |                                    *             \n",
      "         +---------------------------------+                                    *             \n",
      "                  ***            ***                                            *             \n",
      "                **                  **                                          *             \n",
      "              **                      **                                        *             \n",
      "+----------------------+          +-------------+                               *             \n",
      "| VectorStoreRetriever |          | Passthrough |                               *             \n",
      "+----------------------+          +-------------+                               *             \n",
      "                  ***            ***                                            *             \n",
      "                     **        **                                               *             \n",
      "                       **    **                                                 *             \n",
      "         +----------------------------------+                                   *             \n",
      "         | Parallel<context,question>Output |                                   *             \n",
      "         +----------------------------------+                                   *             \n",
      "                           *                                                    *             \n",
      "                           *                                                    *             \n",
      "                           *                                                    *             \n",
      "                +--------------------+                                          *             \n",
      "                | ChatPromptTemplate |                                          *             \n",
      "                +--------------------+                                          *             \n",
      "                           *                                                    *             \n",
      "                           *                                                    *             \n",
      "                           *                                                    *             \n",
      "   +---------------------------------------------+                              *             \n",
      "   | Parallel<completly_processed,prompted>Input |                              *             \n",
      "   +---------------------------------------------+                              *             \n",
      "                   ***           **                                             *             \n",
      "                  *                **                                           *             \n",
      "                **                   **                                         *             \n",
      "        +--------+                     **                                       *             \n",
      "        | Ollama |                      *                                       *             \n",
      "        +--------+                      *                                       *             \n",
      "             *                          *                                       *             \n",
      "             *                          *                                       *             \n",
      "             *                          *                                       *             \n",
      "    +-----------------+          +-------------+                                *             \n",
      "    | StrOutputParser |          | Passthrough |                                *             \n",
      "    +-----------------+          +-------------+                                *             \n",
      "                   ***           **                                             *             \n",
      "                      *        **                                               *             \n",
      "                       **    **                                                 *             \n",
      "   +----------------------------------------------+                 +----------------------+  \n",
      "   | Parallel<completly_processed,prompted>Output |                 | VectorStoreRetriever |  \n",
      "   +----------------------------------------------+                 +----------------------+  \n",
      "                                     ****                      *****                          \n",
      "                                         ****             *****                               \n",
      "                                             **        ***                                    \n",
      "                        +--------------------------------------------------+                  \n",
      "                        | Parallel<completly_and_prompted,retreived>Output |                  \n",
      "                        +--------------------------------------------------+                  \n"
     ]
    }
   ],
   "source": [
    "chain35.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "really really nice. this is exactly what we want. Now add memory\n",
    "https://www.reddit.com/r/LangChain/comments/18yovcm/please_help_with_langchain_want_both_document/.\n",
    "TODO for later: How to convert the retriever output into something nice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding conversation memory to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.utils import ConfigurableFieldSpec\n",
    "from typing import Optional\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> ChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain35,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain35,\n",
    "    RedisChatMessageHistory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history.invoke(\"Where did Harrison work and what did he do?\",\n",
    "                          config={\"configurable\": {\"session_id\": \"foo\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turns out this is legacy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5, memory_key=\"chat_history\")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    memory=memory,\n",
    "    get_chat_history=lambda h : h,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.get_prompts(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Where did Harrison work and what did he do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"where did harrison work?\"\n",
    "history = \"\"\n",
    "result = qa_chain.invoke({\"question\": question, \"chat_history\": history})\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elqmVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
